
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>How can we provide a simple post-hoc explanation for black-box AI models to increase trust and improve model validity? &#8212; AI Practitioner Handbook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7. Solution Delivery" href="../7-solution-delivery/overview.html" />
    <link rel="prev" title="How can we better evaluate time-series classification models?" href="evaluating-timeseries.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-HBHP8472S2"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-HBHP8472S2');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/AISG(R) Horizontal Logo CMYK Full Colour.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">AI Practitioner Handbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to AI Singapore’s AI Practitioner Handbook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1-pre-project-phase/overview.html">
   1. Pre-project Phase
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-pre-project-phase/business_challenge_2_ai_problem.html">
     How does the AI engineer translate the business challenge into an AI problem?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-pre-project-phase/key_areas_in_data.html">
     What are the key areas to look out for in the data when framing the AI project?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-pre-project-phase/technical_debt.html">
     What are the factors/considerations/criteria to consider that will reduce potential technical debt?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1-pre-project-phase/ai_readiness_assessment.html">
     What are some questions that an AI engineer can ask the client during pre-project scoping to assess their AI readiness?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2-proj-mgmt-tech-lead/overview.html">
   2. Project Management &amp; Technical Leadership
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-proj-mgmt-tech-lead/cultivate-ai-team.html">
     How can I cultivate an effective and cohesive AI development team?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-proj-mgmt-tech-lead/principles.html">
     What kind of engineering principles can I set for my development team?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2-proj-mgmt-tech-lead/translate-technical-jargon.html">
     How might we simplify and translate technical jargon for a non-technical audience?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3-collab-dev-platforms/overview.html">
   3. Collaborative Development Platforms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../3-collab-dev-platforms/collaborative_ml.html">
     What are the platforms and their respective considerations required for collaborative ML development?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../3-collab-dev-platforms/repo-structure-setup.html">
     What are some considerations in setting up a project repository to facilitate collaboration and establish good coding practices among developers?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4-lit-review/overview.html">
   4. Literature Review
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../4-lit-review/factors-to-consider-during-literature-review.html">
     What are some of the factors that an AI Engineer should consider during literature review?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5-data-mgmt-exp-proc/overview.html">
   5. Data Management, Exploration &amp; Processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/data-mgmt.html">
     Which data storage options are suitable for the project?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/eda-generic.html">
     Is there a core structure for performing exploratory data analysis in a systematic way?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/data-splits.html">
     What are the various data split strategies?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/data-splits-repeatable.html">
     How do we make data splits repeatable?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/data-splits-bias-fairness-leakage.html">
     What are some potential scenarios of bias, unfairness, data leakage in data splits? How can these be remedied?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/e2e-workflow.html">
     What are the processes involved in building a basic end-to-end workflow?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/e2e-workflow-adv.html">
     How do I enhance the workflow with quality-of-life improvements for my end-to-end workflow?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5-data-mgmt-exp-proc/data-poisoning-data-extraction.html">
     How can I reduce the risks of data poisoning and data extraction?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   6. Modelling
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="optimising-and-satisficing.html">
     What are some internal and external considerations when selecting evaluation metrics?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="model-reproducibility.html">
     How can I maximise model reproducibility?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml-risks-and-robustness.html">
     What are some ML risks I should be aware of? How do ML risks relate to model robustness? What are some tools I can use to assess model robustness?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nlp-metrics.html">
     What are some of the common NLP metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="evaluating-timeseries.html">
     How can we better evaluate time-series classification models?
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     How can we provide a simple post-hoc explanation for black-box AI models to increase trust and improve model validity?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../7-solution-delivery/overview.html">
   7. Solution Delivery
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../7-solution-delivery/deployment-requirements-gathering.html">
     What are some questions to be asked to the project sponsor to understand their deployment requirements?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../7-solution-delivery/min-viable-code.html">
     How can we build a minimum viable code/configuration for CI/CD automation into an existing codebase?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../8-documentation-handover/overview.html">
   8. Documentation &amp; Handover
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../8-documentation-handover/documenting-architecture-processes.html">
     What are some good practices in documenting high-level system architecture and processes of an AI solution?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../_contributing.html">
   How To Contribute
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../markdown-template.html">
     How should I write a section using Markdown?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../notebook-template.html">
     How should I write a section using Jupyter notebooks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../REVIEWING.html">
     How do I review content?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cite.html">
   Cite This Book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../_changelog.html">
   Change Log
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/aimakerspace/ai-practitioner-handbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/aimakerspace/ai-practitioner-handbook/issues/new?title=Issue%20on%20page%20%2Fbook/6-modelling/post-hoc-explanation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/book/6-modelling/post-hoc-explanation.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretable-glass-box-vs-explainable-black-box-models">
   Interpretable (Glass-box) VS Explainable (Black-box) Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-understanding-feature-importance-and-relationship-between-inputs-and-outputs-of-the-model">
     A. Understanding feature importance and relationship between inputs and outputs of the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-explaining-learned-representations-inside-a-neural-network">
     B. Explaining learned representations inside a neural network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>How can we provide a simple post-hoc explanation for black-box AI models to increase trust and improve model validity?</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretable-glass-box-vs-explainable-black-box-models">
   Interpretable (Glass-box) VS Explainable (Black-box) Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-understanding-feature-importance-and-relationship-between-inputs-and-outputs-of-the-model">
     A. Understanding feature importance and relationship between inputs and outputs of the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-explaining-learned-representations-inside-a-neural-network">
     B. Explaining learned representations inside a neural network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="how-can-we-provide-a-simple-post-hoc-explanation-for-black-box-ai-models-to-increase-trust-and-improve-model-validity">
<h1>How can we provide a simple post-hoc explanation for black-box AI models to increase trust and improve model validity?<a class="headerlink" href="#how-can-we-provide-a-simple-post-hoc-explanation-for-black-box-ai-models-to-increase-trust-and-improve-model-validity" title="Permalink to this headline">#</a></h1>
<p>Contributor: Joy Lin, Senior AI Technical Consultant</p>
<hr class="docutils" />
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>Interpretability and explainability have been the recent buzzwords in explainable AI as an attempt to decipher how models derive predictions. As AI is gaining wider acceptance and adoption, stakeholders are increasingly requesting, sometimes under <a class="reference external" href="https://medium.com/womeninai/explainability-as-a-legal-requirement-for-artificial-intelligence-systems-66da5a0aa693">legal requirements</a>, for an explanation on how the model reaches its decision, in order to generate trust in the predictions. Scenarios include: providing a valid reason for rejecting a bank loan, or how various features help in categorising a medical disease.</p>
<p>Explanations come in useful:</p>
<ul class="simple">
<li><p>during model development for error analysis and sense-checking</p></li>
<li><p>after model deployment to increase stakeholder and end user trust, avoiding significant social or financial impact</p></li>
<li><p>during model maintenance to detect drift, bias, or performance changes to guide re-training</p></li>
</ul>
</section>
<section id="interpretable-glass-box-vs-explainable-black-box-models">
<h2>Interpretable (Glass-box) VS Explainable (Black-box) Models<a class="headerlink" href="#interpretable-glass-box-vs-explainable-black-box-models" title="Permalink to this headline">#</a></h2>
<p>There are two broad approaches to deriving explanations, which is tied to the type of model used: interpretability (using glass-box models) or explainability (using black-box models). The table and diagram below shows the distinction between these approaches.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Interpretability (Glass-box)</p></th>
<th class="head"><p>Explainability (Black-box)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Transparency</p></td>
<td><p>Highly translucent</p></td>
<td><p>Opaque</p></td>
</tr>
<tr class="row-odd"><td><p>Model performance</p></td>
<td><p>Less powerful</p></td>
<td><p>More powerful</p></td>
</tr>
<tr class="row-even"><td><p>Relationship to model</p></td>
<td><p>Model-specific</p></td>
<td><p>Model-agnostic</p></td>
</tr>
<tr class="row-odd"><td><p>Derivation of explanations</p></td>
<td><p>Via model’s functional form</p></td>
<td><p>Via post-hoc analysis of model inputs and outputs</p></td>
</tr>
<tr class="row-even"><td><p>Outcome</p></td>
<td><p>Exact explanation</p></td>
<td><p>Approximate explanation</p></td>
</tr>
<tr class="row-odd"><td><p>Examples</p></td>
<td><p>Linear regression model coefficients</p></td>
<td><p>Permutation feature importance ranking</p></td>
</tr>
</tbody>
</table>
<p><img alt="glassbox-blackbox" src="../../_images/glassbox-blackbox.png" />
<em><a class="reference external" href="https://interpret.ml/">[Source]</a></em></p>
<p>With glass-box models, explanations are inherent to the model, and exact.</p>
<p>With black-box models, explanations require additional post-hoc analysis of the trained model, and explanations are approximate only. Due to this, they need to be used with caution as explanations may vary between runs or when input data changes slightly.</p>
<p>In this article, we focus only on explainability methods for black-box models.</p>
<section id="a-understanding-feature-importance-and-relationship-between-inputs-and-outputs-of-the-model">
<h3>A. Understanding feature importance and relationship between inputs and outputs of the model<a class="headerlink" href="#a-understanding-feature-importance-and-relationship-between-inputs-and-outputs-of-the-model" title="Permalink to this headline">#</a></h3>
<p>Global model-agnostic methods describe how input features affect the output prediction <em>on average</em>. In particular, these methods tell us i) the importance of each feature, and ii) how it impacts the prediction (positive, neutral, or negative). They can be applied to black-box models like tree-based models and neural networks.</p>
<p>Examples:</p>
<ol class="simple">
<li><p>Permutation feature importance: measures the importance of a feature as an increase in loss when the feature is permuted.
<img alt="importance" src="../../_images/importance.png" />
<em><a class="reference external" href="https://docs.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/mlx/permutation_importance.html#:~:text=Feature%20permutation%20importance%20measures%20the,to%20measure%20the%20prediction%20error.">[Source]</a> In the Titanic dataset, passenger gender is the most important feature to survival outcome, and about twice as important as passenger class and age.</em></p></li>
<li><p>SHAP (SHapley Additive exPlanations): globally estimates the contribution of every input feature with a combination of high/low and positive/negative Shapley value(s). Useful for sense-checking feature-target relationship.
<img alt="shapley_global" src="../../_images/shapley_global.png" />
<em><a class="reference external" href="https://www.analyticsvidhya.com/blog/2019/11/shapley-value-machine-learning-interpretability-game-theory/">[Source]</a> Summary plot of feature importance and feature effects, where each dot represents an instance per feature.</em></p></li>
<li><p>Partial Dependence Plot (PDP): shows the marginal impact that one or two features have on the predictions. Useful to deep dive and understand the direction of the relationship between specific feature(s) and the target, although limited to easy visualisation of maximum two features.
<img alt="pdp" src="../../_images/pdp.jpeg" />
<em><a class="reference external" href="https://christophm.github.io/interpretable-ml-book/pdp.html">[Source]</a> PDP of cancer probability VS the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared to women who had 0 or more than 2 pregnancies. Caution: These relationships should not be interpreted causally.</em></p></li>
</ol>
</section>
<section id="b-explaining-learned-representations-inside-a-neural-network">
<h3>B. Explaining learned representations inside a neural network<a class="headerlink" href="#b-explaining-learned-representations-inside-a-neural-network" title="Permalink to this headline">#</a></h3>
<p>A neural network can have multiple nodes and layers where input data undergo complex mathematical operations to output predictions. However increasing network density makes it nearly impossible to represent these complexities in a human-readable way. Instead, we use visual representations of data inside a neural network to understand the predictions.</p>
<p>Examples:</p>
<ol>
<li><p>Learned features</p>
<p>A model can learn - with increasing complexity across the network layers - the various edges, textures, patterns, parts, and eventually objects in images. Assessing these network layers helps you understand the features learned (or missed).</p>
<p><img alt="features" src="../../_images/features.png" />
<em><a class="reference external" href="https://distill.pub/2017/feature-visualization/">[Source]</a></em></p>
<p>Similarly, models can learn features from text or tabular data.</p>
</li>
<li><p>Pixel attribution using saliency maps</p>
<p>Saliency maps provide another visualisation using ranked coloured pixels to indicate their contribution to the model prediction.
<img alt="saliency_map" src="../../_images/saliency_map.png" />
<em><a class="reference external" href="https://usmanr149.github.io/urmlblog/cnn/2020/05/01/Salincy-Maps.html">[Source]</a> Input image (left) and corresponding saliency map (right).</em></p>
</li>
</ol>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://christophm.github.io/interpretable-ml-book/simple.html">Glass-box models (model-specific)</a></p></li>
<li><p><a class="reference external" href="https://christophm.github.io/interpretable-ml-book/agnostic.html">Black-box models (model-agnostic)</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./book/6-modelling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="evaluating-timeseries.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">How can we better evaluate time-series classification models?</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../7-solution-delivery/overview.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">7. Solution Delivery</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By AI Singapore<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>